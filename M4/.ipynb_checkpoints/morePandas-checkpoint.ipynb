{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Pandas\n",
    "\n",
    "Admittedly, like almost everything in life, it will take time and practice to become proficient with a skill such as utilizing `pandas`. Unfortunately, we do not have an unlimited amount of time to spend learning and practicing all of the useful features of `pandas`. Now we try to get a bit more practice on the tasks that you will utilize frequently:\n",
    "\n",
    "- reading in data from files\n",
    "- exploring data both \"statistically\" and visually\n",
    "- slicing data\n",
    "- cleaning data\n",
    "- exporting or saving your cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Heart Disease Data Set\n",
    "\n",
    "We'll look at classic machine learning dataset concerning [heart disease][1]. The `.csv` file is called `heart.csv` and has the following data dictionary:\n",
    "\n",
    "- age: the age of the patient in years\n",
    "- sex: sex\n",
    "  - 1 = male\n",
    "  - 0 = female\n",
    "- chestPain: type of chest pain\n",
    "  - typical angina\n",
    "  - atypical angina\n",
    "  - non-anginal pain\n",
    "  - asymptomatic\n",
    "- restBP: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol: serum cholesterol in mg/dl\n",
    "- fbs: fasting blood sugar > 120 mg/dl\n",
    "  - 1 = true\n",
    "  - 0 = false \n",
    "- restECG: resting electrocardiographic results \n",
    "  - 0 = normal\n",
    "  - 1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "  - 2 = showing probably or definite left ventricular hypertrophy by Estes' criteria\n",
    "- maxHR: maximum heart rate achieved\n",
    "- exAng: exercise induced angina\n",
    "  - 1 = yes\n",
    "  - 0 = no\n",
    "- oldPeak: ST depression induced by exercise relative to rest\n",
    "- slope: the slope of teh peak exercise ST segment\n",
    "  - 1 = upsloping\n",
    "  - 2 = flat\n",
    "  - 3 = downsloping\n",
    "- ca: number of major vessels (0-3) colored by flourosopy\n",
    "- thal: Thalium stress test result\n",
    "  - normal\n",
    "  - fixed defect\n",
    "  - reversable defect\n",
    "- hd: heart disease?\n",
    "  - No\n",
    "  - Yes\n",
    "\n",
    "----\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the file \n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# and take a peek with head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many rows and columns we have\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Data Set\n",
    "\n",
    "We have a 303 rows of data (i.e., observations) and 14 columns (i.e., attributes). We just saw that we can see the top five rows by calling `.head()`. We can also see the bottom rows with `.tail()`. Many times it is also useful to to look at the *sample* of the data. We can easily do so with the `.sample()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample of 10 rows of data\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "We can get some basic summary statistics for all of the **numerical** columns by using the method `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .describe() to get summary statistics of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Revisiting `value_counts()`\n",
    "\n",
    "If you recall, we could use the the method `.value_counts()` on a `Series` object that contained  categorical data to see the frequency of each value. Since version 1.1.0 of `pandas` you can now also call `.value_counts()` on a `DataFrame`. When you do so, it returns the count of unique rows in the `DataFrame`. In many cases, this doesn't help us too much. However, it can be useful if you think you have duplicate rows in the data.\n",
    "\n",
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call value_counts() on the DataFrame\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we surmised, this doesn't help us much here. We do see that there are 297 unique rows out of the 303. This could suggest several things, but my gut is telling me that there may be missing data for some of the columns. Before we formally explore that idea, let's call `value_counts()` on the subset of columns that are categorical: `chestPain`, `thal`, and `hd`. Here it is important to determine if we want the frequency for each of the categorical columns *separately* or if we want them grouped together in some fashion.\n",
    "\n",
    "Let's start with them separately, then look at them grouped together to get a sense of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the categorical columns for convenience\n",
    "catCols = [\"chestPain\", \"thal\", \"hd\"]\n",
    "\n",
    "# Loop over each categorical column and print out the value counts\n",
    "for col in catCols:\n",
    "    print(df[col].value_counts())\n",
    "    \n",
    "# Counts are great, but sometimes we want proportions/percentages instead\n",
    "print(\"\\nProportions instead:\")\n",
    "for col in catCols:\n",
    "    print(df[col].value_counts(normalize=True))\n",
    "    \n",
    "# Now look at the counts grouped together by all categorical variables\n",
    "df[[\"chestPain\", \"thal\", \"hd\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Information on Each Column\n",
    "\n",
    "Additionally, we can get some useful information about the `DataFrame` by using the method `.info()`. It will return the type of index along with a bit of information about it and information about each of the columns: the name, a count of non-null values, and the data type. A quick glance at the results helps you understand if there are missing values as well as if any of the columns are of data type that you did not expect. The resulting counts from `.info()` are a bit more helpful than the ones you get with `.describe()` because `.info()` includes **all** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .info() get some basic information about the columns of the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values?\n",
    "\n",
    "You can determine how rows have missing values (i.e., null values) from the results above. In this case, you can do the calculations easily in your head. What if, instead, you have a lot more rows of data? That arithmetic may not be quite as easy. Luckily, you can write one line of code that will do the math for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the missing values for each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "We see that the column `ca` has 4 missing values and the column `thal` has two missing values. Note that we do **not** know if the missing values for the two columns are from the same rows. So, worst case, we have a total of 6 rows that have missing data. \n",
    "\n",
    "The question then becomes what should you do about it? Depending on what you plan to do with the data, it may be a (big) problem. For example, specific machine learning techniques require that you have no missing data. So, what are your options for dealing with the missing data? *Note: which of these options you decide to use is **highly** dependent on your specific business scenario and application.*\n",
    "\n",
    "1. You could delete the rows that contain missing values.\n",
    "2. You could **impute** the missing values (i.e., fill in the \"holes\").\n",
    "  - Using the mean of the column.\n",
    "  - Using the median of the column.\n",
    "  - Using the mode of the column.\n",
    "  - A value from another randomly selected record.\n",
    "  - A value estimated by another predictive model.\n",
    "  - A constant value that some meaning in the context within the domain, distinct from all other values.\n",
    "  \n",
    "You should be aware that some of these options only really work for numerical columns. For example, what is the \"average\" of a categorical variable?\n",
    "\n",
    "In general, it is good practice to make a copy of the original `DataFrame` and then manipulate/change the copy. This allows you to always go back to the original data in case you need to start over.\n",
    "\n",
    "Let's start by deleting the missing values with the `.dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame\n",
    "heart = df.copy()\n",
    "\n",
    "# We can drop all of the null values \n",
    "heart.dropna(inplace=True)\n",
    "heart.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we lost a total of 6 rows by deleting the missing values.\n",
    "\n",
    "-----\n",
    "\n",
    "Now, let's try to make another copy of the original `DataFrame` and fill in the missing values with the mean of the specific column. Recall that the columns `ca` and `thal` were the columns with missing data. Also, remember that `thal` is a non-numerical column. This means we can only impute the missing values of `ca` by using the average of the column.\n",
    "\n",
    "*Note: The astute student will say, \"Hey, wait a minute! You shouldn't do that!\" Are you an astute student?*\n",
    "\n",
    "Regardless of whether you object or not, I **am** going to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make another copy of df\n",
    "heart2 = df.copy()\n",
    "\n",
    "# Then verify that we have missing values in ca and thal\n",
    "heart2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is useful to see which columns are the ones with missing data. What if we are interested in the **rows** of data that are affected? We easily filter the `DataFrame` by sending it the an list of `boolean` values that tell us whether or not a row contains a missing value.\n",
    "\n",
    "First, create and look at the `boolean` list. Then, send that list as a selection criteria to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll build up to the final result in steps so you see what is happening.\n",
    "# Calling .isnull() on a DataFrame will return a a boolean same-sized object\n",
    "# indicating if the values are missing (True)\n",
    "heart2.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember when we call .values we get back an numpy.ndarray\n",
    "print(f\"type(heart2.isnull().values): {type(heart2.isnull().values)}\")\n",
    "\n",
    "# From numpy we can call .any(axis=1) on the ndarray\n",
    "# This will test to see if any value is True along the given axis\n",
    "# In our case, axis=1 says \"Check the column for missing values\"\n",
    "heart2.isnull().values.any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, putting it all together to see the rows with missing data\n",
    "heart2[heart2.isnull().values.any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Back to Imputing with the Mean\n",
    "\n",
    "Having the rows of missing data may be very informative. Now, let's get back to filling in the missing values for `ca` with the average of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the average of the ca column just to see what it is\n",
    "print(f\"average of ca   = {heart2['ca'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill in the missing values with fillna()\n",
    "heart2[\"ca\"].fillna(value=heart2[\"ca\"].mean(), inplace=True)\n",
    "\n",
    "# spit out the info() again\n",
    "heart2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the column `ca` now has 303 non-null values and the data type is `float64`.  What if we want to see the rows that we imputed. We can find those rows by comparing the values of the column to the average of the column.\n",
    "\n",
    "*Thought Exercise: Do these rows correspond to the ones we found above? Why does this work?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart2.loc[heart2[\"ca\"] == heart2[\"ca\"].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "If you want impute a numerical column with median or mode instead of the average, then the process is identical to what we just did. The only difference is you simply use the desired statistical function instead of `.mean()`.\n",
    "\n",
    "Sometimes we want to fill in the missing data with the same value that is above/before or below/after the row that contains the missing data. (You will see this used in time series where a record is missing a timestamp.) We can use the `.fillna()` method to accomplish this task. There are aliased methods available also: `ffill()` and `bfill()`. \n",
    "\n",
    "Let's try both of forward-filling and back-filling with the `thal` column. From above, we know that index 87 and 266 are the rows that have missing values for `thal`. Let's look at each of those sections of the data prior to filling them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the row above and below the first missing data row\n",
    "heart2.iloc[86:89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the row above and below the second missing data row\n",
    "heart2.iloc[265:268]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case we see that both the row above and below have the same value: `normal`. In the second case, we have a similar situation where the row above and below are the same value of `fixed`. In both cases it does not matter if we use forward or backward filling since the result will be the same. We will arbitrarily use forward filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill in place\n",
    "heart2.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the filled in data\n",
    "heart2.iloc[86:89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the filled in data\n",
    "heart2.iloc[265:268]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "There is no \"simple\" way to limit the number holes that get filled in with the `.fillna()` method. There is a parameter called `limit`, but it only works for continuous blocks of missing values in the column. We did not have this situation in our data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "\n",
    "We often, especially when building mathematical models, need to convert categorical variables into numbers. This process is called *encoding* and there are various ways to accomplish it. We have two basic options to accomplish this task using `pandas`. If you are building machine learning models, we may instead wish to use the options provided by the package/module scikit-learn in order to build the *pipeline* easier. For now, we'll look at the two options provided in `pandas`.\n",
    "\n",
    "Option 1: `factorize` - the result will be **1 dimension** with numerical values for that one column/dimension.\n",
    "\n",
    "Option 2: `get_dummies` - the result will be **$n$ dimensions** where $n$ is the number of categories. You will have $n$ new columns where a 1 indicates that it is in that category and a 0 otherwise.\n",
    "\n",
    "Let's examine both techniques. We'll start with the column `hd` which has only two possible values: No and Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets' make a copy of heart2\n",
    "heart3 = heart2.copy()\n",
    "\n",
    "# This copy should have no missing data since we imputed the data above\n",
    "heart3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .factorize() with the column we want to convert\n",
    "# Just see what the result is\n",
    "pd.factorize(heart3[\"hd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `tuple` (*how do you know this?*). The first element is an array containg the numerical result of 0s and 1s. The second elements tells us what each numerical value maps to. In this case, 0 = \"No\" and 1 = \"Yes\", which is the common interpretation of a binary variable.\n",
    "\n",
    "Now let's look at `get_dummies`. We expect two new columns because there are two possible values for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(heart3[\"hd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we do indeed get two new columns. When the value 1 is in the \"No\" column it means that row had an original `hd` value of \"No\". When the value 0 is in the \"No\" column it means that row had an original `hd` value of something **other than \"No\"**. In this case, the only other option is \"Yes\".\n",
    "\n",
    "Which of these two options do you think we should use to replace our current column of `hd`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at head of heart3 before manipulation\n",
    "heart3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace hd by using factorize\n",
    "# We only want the 0,1 array\n",
    "heart3[\"hd\"] = pd.factorize(heart3[\"hd\"])[0]\n",
    "heart3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to have worked correctly. Good job!\n",
    "\n",
    "Now, what about the other categorical columns: `chestPain` and `thal`? The selection of using `factorize` or `get_dummies` **greatly depends on what you plan to do with the data and what type of modeling you want to attempt.** In general, if your have strictly **categorical** variable, you can use `factorize` to give each category a unique label/number. If, instead, you have an **ordinal** variable, then you want to make sure the that numerical values given to each category is in the proper order. (Think of a Likert scale here.) If you are trying to tease out the differences between the different categories in, say, a linear regression model, then you will often create the *dummy* variables. For example, in a time series forecasting linear regression model, you can capture seasonality by creating dummy variables for each season.\n",
    "\n",
    "Just for fun, let's convert `chestPain` using `factorize` and `thal` using `get_dummies`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the different values and frequency of each\n",
    "heart3[\"chestPain\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that when we call a factorize we get a tuple\n",
    "# If we are interested in being able to re-establish what each numerical value\n",
    "# corresponds to, we can capture the index like below\n",
    "heart3[\"chestPain\"], cpMapping = pd.factorize(heart3[\"chestPain\"])\n",
    "heart3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original values back\n",
    "cpMapping.take(heart3[\"chestPain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the different values and frequency of each as numbers\n",
    "heart3[\"chestPain\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the different values and frequency of each mapping back to words\n",
    "cpMapping.take(heart3[\"chestPain\"]).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use factorize to replace chestPain\n",
    "heart3[\"chestPain\"] = pd.factorize(heart3[\"hd\"])[0]\n",
    "heart3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create dummy variables for the column `thal`. We will have $n$ different new columns. In many models (e.g., multiple linear regression), you cannot have all of $n$ dummy variables because of multicollinearity. Instead you need to have $n-1$ dummy variables, where the \"missing\" one is the **base** to which all of the others are compared. \n",
    "\n",
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the DataFrame again\n",
    "heart3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dummy variables for just the column thal\n",
    "# It will return a new DataFrame with n columns\n",
    "pd.get_dummies(heart3[\"thal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if I send it the entire DataFrame?\n",
    "pd.get_dummies(heart3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it drops the original column of `thal` and creates $n$ new columns with the prefix `thal_`. If there had still been another categorial variable(s) in the `DataFrame` then those would have reacted the same way. As indicated above, we often only want $n-1$ of the dummy variables. To achieve this we can pass the parameter `drop_first=True` to the method. Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if I send it the entire DataFrame?\n",
    "pd.get_dummies(heart3, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, now that we only have 2 columns for the original `thal` variable. The first row has a 0 for both `thal_normal` and `thal_reversable`. This means that the first row must have had the value of `fixed` for `thal`. The value/category of `fixed` has been created as the **base**. Suppose we are not satisfied with our encoding. Our next step is to save this result into a new variable to be used for modeling purposes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies and save to new variable called heart4\n",
    "heart4 = pd.get_dummies(heart3, drop_first=True)\n",
    "heart4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to save our cleaned data to a file so that we can give it all of our friends in hopes that they will claim we are the best coder they have ever seen. We can easily export our data to various data formats with `pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export heart4 as a .csv file WITHOUT the index\n",
    "heart4.to_csv(r\"newHeart.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export heart4 as a .xlsx file WITHOUT the index\n",
    "heart4.to_excel(r\"newHeart.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "You have been given an Excel file with the order history of our Super Store that has location in the United States. In the **Code** cells below, perform the following tasks in the appropriate cell.\n",
    "\n",
    "1. Read in the data from `superStore.xlsx` into a `DataFrame` called `ss` and look at the column information.\n",
    "2. Sample 5 rows of `ss` to see what the data looks like.\n",
    "3. Print out the summary statistics for `ss`.\n",
    "4. Identify the top 3 states in terms of total profit generated.\n",
    "5. Create a bar plot consisting of all the states' profits. What do you notice?\n",
    "6. What are the top 3 products in terms of total profit from the state with the highest overall profit?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in your data from the file superStore.xlsx and look at the column information\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sample 5 rows of the data\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get summary statistics for your data\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Identify the top 3 states in terms of profit generated\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a bar plot of all the states' total profit\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Using only top profit-generating state, what are the top 3 products?\n",
    "### YOUR CODE HERE\n",
    "# Hint 1 - create a new DataFrame with only rows for the top state\n",
    "\n",
    "# Hint 2 - then use the new DataFrame to find top 3 products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2021 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
